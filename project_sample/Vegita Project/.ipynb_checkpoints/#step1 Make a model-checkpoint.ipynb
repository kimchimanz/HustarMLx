{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---------\n",
    "# 배추가격 예측 AI Project for 모인활\n",
    "\n",
    "Notes by kjune18@naver.com, Handong Global Univ."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 기대효과"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 선형 회귀까지만 공부한 사람도 실제로 기상청 등의 데이터에서 다양한 배추 가격 데이터를 수집하고 직접 활용하여 스스로 웹사이트까지 구축해볼 수 있다.\n",
    "- 결과적으로 사용자의 입력값에 따라 적당한 배추 가격을 예측해주는 모델을 만들고, 모델을 통해 결과를 보여줄 수 있다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 포함되는 개념들"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. 파이썬 and Tensorflow에서 학습된 모델을 저장하고 필요에 따라서 불러오는 기능\n",
    "2. Flask Web Server & Mdbootstrap Design Framework"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. 선형 회귀\n",
    "\n",
    "선형 회귀는 통계학에서 종속 변수 $y$와 한 개 이상의 독립 변수 $x$와의 선형 상관 관계를 모델링하는 회귀분석 기법이다.\n",
    "선형 회귀는 선형 예측 함수를 사용해 회귀식을 모델링하며, 알려지지 않은 파라미터는 데이터로부터 추정한다. 이렇게 만들어진 회귀식을 선형 모델이라고 한다.\n",
    "\n",
    "선형 회귀는 여러 사용 사례가 있지만, 대개 아래와 같은 두 가지 분류 중 하나로 요약할 수 있다.\n",
    "- 값을 예측하는 것이 목적일 경우, 선형 회귀를 사용해 데이터에 적합한 예측 모형을 개발한다. 개발한 선형 회귀식을 사용해 $y$가 없는 $x$값에 대해 $y$를 예측하기 위해 사용할 수 있다.\n",
    "\n",
    "\n",
    "- 종속 변수 $y$와 이것과 연관된 독립 변수 $X_1$,  ...   ,$X_p$\n",
    "가 존재하는 경우에, 선형 회귀 분석을 사용해 $X_j$와 $y$의 관계를 정량화할 수 있다.\n",
    "\n",
    "\n",
    "<img src = \"https://upload.wikimedia.org/wikipedia/commons/b/be/Normdist_regression.png\" width=300>\n",
    "<center>그림 1: 독립변수 1개와 종속변수 1개를 가진 선형 회귀의 예"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. 데이터 수집"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "우리나라 정부에서는 다양한 데이터를 무료로 제공해주고 있다. 그러나, 데이터를 제공하는 정책이 서로 다를 수 있기 때문에 데이터를 수집하는 방법에 있어서는 조금씩 다르게 활용할 필요가 있다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 기상청 자료 이용하기"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. 기상자료개방포털(기후통계분석)로 이동한다.\n",
    "2. 먼저 기온분석으로 이동하여, 원하는 기간과 지역/지점을 선택하고 csv다운로드 버튼을 눌러 데이터를 얻도록 한다.\n",
    "3. 다음으로 강수량분석으로 이동하여, 앞에서 받은 데이터와 동일한 기간, 동일한 지역/지점을 선택하고 csv다운로드 버튼을 통해 데이터를 얻도록 한다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 농산물유통 자료 이용하기"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. 농산물유통정보사이트로 이동한다.\n",
    "2. 가격정보에서 채소류->배추를 선택하고, 품종과 등급은 전체로 하여 조회한다.\n",
    "3. 데이터 저장버튼을 통해 데이터를 얻도록 한다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. 데이터 정제"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "처음 수집한 data는 인공지능이 인식하기 쉬운 형태는 아니다.\n",
    "따라서 엑셀 내부의 data를 정제함으로써 인공지능이 인식하기 쉬운 형태로 만들어 줘야 한다.\n",
    "특히 배추의 가격은 주말 등에는 측정되지 않는 점이 있고, 몇몇 일자에는 가격이 급상승하는 data가 있기 때문에 data의 정제가 반드시 필요하다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "여기에서 사용할 변인은 평균온도, 최저온도, 최고온도, 강수량 총 4가지 변인을 사용하여 가격을 예측하도록 할 것이다.\n",
    "<img src = \"https://t1.daumcdn.net/cfile/tistory/99D710465B7D9A8E18\" width= 300>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "정제된 data 링크 https://github.com/returnb2b/AI-for-All/blob/main/Projects/Vegita_Project/price%20data.csv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. 모델 만들기"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "다변인 선형 회귀는 모델에 영향을 미치는 변인이 여러 개 일때 사용하는 모델이다. 현재 우리의 데이터에서는 변인이 '평균온도', '최저온도', 최고온도', '강수량'이므로 이 모든 요인이 '가격'에 영향을 미친다고 감안해야 한다. 따라서 가중치(Weight)을 고려했을 때 다음과 같은 수식을 세울 수 있다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$H(x_1, x_2, x_3, x_4) = x_1w_1 + x_2w_2 + x_3w_3 + x_4w_4$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "파이썬에서 Tensorflow 라이브러리를 활용해 다변인 선형 회귀 모델을 구현한다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.3.0\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "print(tf.version.VERSION)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "module 'tensorflow' has no attribute 'global_variables_initializer'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-6-cb623de75d73>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[1;31m#tf.disable_v2_behavior()\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      7\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 8\u001b[1;33m \u001b[0mmodel\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mglobal_variables_initializer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m   \u001b[1;31m# model 초기화하기\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      9\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     10\u001b[0m \u001b[0mdata\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mread_csv\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'price data.csv'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msep\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m','\u001b[0m\u001b[1;33m)\u001b[0m  \u001b[1;31m# csv파일 읽어오기\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mAttributeError\u001b[0m: module 'tensorflow' has no attribute 'global_variables_initializer'"
     ]
    }
   ],
   "source": [
    "# 엑셀에서 data를 읽어온다.\n",
    "# pandas library: excel data를 읽어올 수 있다.\n",
    "#import tensorflow.compat.v1 as tf\n",
    "import numpy as np\n",
    "from pandas.io.parsers import read_csv\n",
    "#tf.disable_v2_behavior()\n",
    "\n",
    "model = tf.global_variables_initializer()   # model 초기화하기\n",
    "\n",
    "data = read_csv('price data.csv', sep=',')  # csv파일 읽어오기\n",
    "\n",
    "xy = np.array(data, dtype=np.float32)    # 행렬 형태로 저장하기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4개의 변인을 입력을 받는다.\n",
    "x_data = xy[:, 1:-1]   # slicing x\n",
    "print(x_data)\n",
    "# 가격 값을 입력 받는다.\n",
    "y_data = xy[:, [-1]]   # slicing y\n",
    "print(y_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "placeholder의 전달 파라미터는 다음과 같다.\n",
    "\n",
    "placeholder(\n",
    "    dtype,\n",
    "    shape=None,\n",
    "    name=None\n",
    ")\n",
    "\n",
    "\n",
    "- dtype : 데이터 타입을 의미하며 반드시 적어주어야 한다.\n",
    "\n",
    "- shape : 입력 데이터의 형태를 의미한다. 상수 값이 될 수도 있고 다차원 배열의 정보가 들어올 수도 있다. ( 디폴트 파라미터로 None 지정 )\n",
    "\n",
    "- name : 해당 placeholder의 이름을 부여하는 것으로 적지 않아도 된다.  ( 디폴트 파라미터로 None 지정 )\n",
    "\n",
    "텐서플로우 random_normal():\n",
    "0~1 사이의 정규확률분포 값을 생성해주는 함수.  \n",
    "원하는 shape 대로 만들어줌"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "# placeholder를 설정한다.\n",
    "# Tensorflow2에선 생략\n",
    "\n",
    "X = tf.placeholder(tf.float32, shape=[None, 4])  # 변인 4개\n",
    "Y = tf.placeholder(tf.float32, shape=[None, 1])  # 변인 1개\n",
    "W = tf.Variable(tf.random_normal([4, 1]), name=\"weight\")\n",
    "b = tf.Variable(tf.random_normal([1]), name=\"bias\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "W(가중치) 값과 b(바이어스) 의 값을 지정해야 하는데 이는 학습을 위해 임의의 값 (아직 잘 모른다고 가정)을 던져준다.\n",
    "\n",
    "텐서플로우에서는 W와 b에 해당하는 변수를 Variable 을 사용하여 나타낸다. 그리고 텐서 플로우가 알아서 이 값을 자동으로 조절해 줄 것이며 나중에 비용함수가 최소(minimize)가 되는 가중치와 바이어스 값으로 조정하게 된다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 가설을 설정한다.\n",
    "hypothesis = tf.matmul(X, W) + b  # 행렬 곱 연산"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 비용함수를 설정한다.\n",
    "# mse(mean squared error)\n",
    "cost = tf.reduce_mean(tf.square(hypothesis - Y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 최적화 함수를 설정한다. 여기선 GradientDescent(경사하강) 함수 사용\n",
    "# 학습률 0.000005로 설정한 이유 - 되도록 짧은 시간에 정확한 결과 나오게 하기 위함\n",
    "# 경사하강의 간격\n",
    "optimizer = tf.train.GradientDescentOptimizer(learning_rate=0.000005)\n",
    "train = optimizer.minimize(cost)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 세션을 생성한다.\n",
    "sess = tf.Session()\n",
    "\n",
    "# 글로벌 변수 초기화시킨다.\n",
    "sess.run(tf.global_variables_initializer())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# 0  손실비용:  12702171.0\n",
      "-배추 가격:  [1.0223333]\n",
      "# 500  손실비용:  4198919.5\n",
      "-배추 가격:  [-279.83792]\n",
      "# 1000  손실비용:  3791032.8\n",
      "-배추 가격:  [30.44785]\n",
      "# 1500  손실비용:  3472610.5\n",
      "-배추 가격:  [313.15082]\n",
      "# 2000  손실비용:  3222243.0\n",
      "-배추 가격:  [564.31134]\n",
      "# 2500  손실비용:  3024890.5\n",
      "-배추 가격:  [787.4435]\n",
      "# 3000  손실비용:  2869069.5\n",
      "-배추 가격:  [985.7306]\n",
      "# 3500  손실비용:  2745902.5\n",
      "-배추 가격:  [1161.9795]\n",
      "# 4000  손실비용:  2648472.8\n",
      "-배추 가격:  [1318.6677]\n",
      "# 4500  손실비용:  2571358.0\n",
      "-배추 가격:  [1457.9865]\n",
      "# 5000  손실비용:  2510294.8\n",
      "-배추 가격:  [1581.874]\n",
      "# 5500  손실비용:  2461921.2\n",
      "-배추 가격:  [1692.0518]\n",
      "# 6000  손실비용:  2423584.2\n",
      "-배추 가격:  [1790.0427]\n",
      "# 6500  손실비용:  2393186.8\n",
      "-배추 가격:  [1877.2002]\n",
      "# 7000  손실비용:  2369071.0\n",
      "-배추 가격:  [1954.7269]\n",
      "# 7500  손실비용:  2349926.2\n",
      "-배추 가격:  [2023.6893]\n",
      "# 8000  손실비용:  2334715.0\n",
      "-배추 가격:  [2085.0352]\n",
      "# 8500  손실비용:  2322616.8\n",
      "-배추 가격:  [2139.6084]\n",
      "# 9000  손실비용:  2312982.0\n",
      "-배추 가격:  [2188.1572]\n",
      "# 9500  손실비용:  2305297.0\n",
      "-배추 가격:  [2231.3472]\n",
      "# 10000  손실비용:  2299155.5\n",
      "-배추 가격:  [2269.7717]\n",
      "# 10500  손실비용:  2294235.0\n",
      "-배추 가격:  [2303.9558]\n",
      "# 11000  손실비용:  2290281.0\n",
      "-배추 가격:  [2334.3694]\n",
      "# 11500  손실비용:  2287092.0\n",
      "-배추 가격:  [2361.4275]\n",
      "# 12000  손실비용:  2284508.8\n",
      "-배추 가격:  [2385.5017]\n",
      "# 12500  손실비용:  2282405.0\n",
      "-배추 가격:  [2406.921]\n",
      "# 13000  손실비용:  2280680.0\n",
      "-배추 가격:  [2425.98]\n",
      "# 13500  손실비용:  2279255.5\n",
      "-배추 가격:  [2442.9368]\n",
      "# 14000  손실비용:  2278068.8\n",
      "-배추 가격:  [2458.025]\n",
      "# 14500  손실비용:  2277069.8\n",
      "-배추 가격:  [2471.4507]\n",
      "# 15000  손실비용:  2276220.5\n",
      "-배추 가격:  [2483.3965]\n",
      "# 15500  손실비용:  2275488.2\n",
      "-배추 가격:  [2494.027]\n",
      "# 16000  손실비용:  2274850.0\n",
      "-배추 가격:  [2503.487]\n",
      "# 16500  손실비용:  2274285.5\n",
      "-배추 가격:  [2511.9058]\n",
      "# 17000  손실비용:  2273779.5\n",
      "-배추 가격:  [2519.3982]\n",
      "# 17500  손실비용:  2273320.0\n",
      "-배추 가격:  [2526.0664]\n",
      "# 18000  손실비용:  2272897.2\n",
      "-배추 가격:  [2532.0005]\n",
      "# 18500  손실비용:  2272503.5\n",
      "-배추 가격:  [2537.282]\n",
      "# 19000  손실비용:  2272132.8\n",
      "-배추 가격:  [2541.985]\n",
      "# 19500  손실비용:  2271780.5\n",
      "-배추 가격:  [2546.1702]\n",
      "# 20000  손실비용:  2271443.0\n",
      "-배추 가격:  [2549.8965]\n",
      "# 20500  손실비용:  2271117.0\n",
      "-배추 가격:  [2553.2144]\n",
      "# 21000  손실비용:  2270800.0\n",
      "-배추 가격:  [2556.1687]\n",
      "# 21500  손실비용:  2270490.0\n",
      "-배추 가격:  [2558.7983]\n",
      "# 22000  손실비용:  2270186.5\n",
      "-배추 가격:  [2561.142]\n",
      "# 22500  손실비용:  2269887.2\n",
      "-배추 가격:  [2563.2268]\n",
      "# 23000  손실비용:  2269591.8\n",
      "-배추 가격:  [2565.0889]\n",
      "# 23500  손실비용:  2269299.5\n",
      "-배추 가격:  [2566.7417]\n",
      "# 24000  손실비용:  2269009.8\n",
      "-배추 가격:  [2568.2173]\n",
      "# 24500  손실비용:  2268721.5\n",
      "-배추 가격:  [2569.5327]\n",
      "# 25000  손실비용:  2268435.0\n",
      "-배추 가격:  [2570.7048]\n",
      "# 25500  손실비용:  2268149.8\n",
      "-배추 가격:  [2571.75]\n",
      "# 26000  손실비용:  2267865.5\n",
      "-배추 가격:  [2572.6858]\n",
      "# 26500  손실비용:  2267582.8\n",
      "-배추 가격:  [2573.5225]\n",
      "# 27000  손실비용:  2267299.8\n",
      "-배추 가격:  [2574.2603]\n",
      "# 27500  손실비용:  2267017.5\n",
      "-배추 가격:  [2574.9133]\n",
      "# 28000  손실비용:  2266736.5\n",
      "-배추 가격:  [2575.519]\n",
      "# 28500  손실비용:  2266456.0\n",
      "-배추 가격:  [2576.048]\n",
      "# 29000  손실비용:  2266175.5\n",
      "-배추 가격:  [2576.5369]\n",
      "# 29500  손실비용:  2265895.0\n",
      "-배추 가격:  [2576.9631]\n",
      "# 30000  손실비용:  2265615.0\n",
      "-배추 가격:  [2577.3394]\n",
      "# 30500  손실비용:  2265336.0\n",
      "-배추 가격:  [2577.678]\n",
      "# 31000  손실비용:  2265057.2\n",
      "-배추 가격:  [2578.0037]\n",
      "# 31500  손실비용:  2264778.5\n",
      "-배추 가격:  [2578.253]\n",
      "# 32000  손실비용:  2264499.5\n",
      "-배추 가격:  [2578.4946]\n",
      "# 32500  손실비용:  2264221.0\n",
      "-배추 가격:  [2578.7358]\n",
      "# 33000  손실비용:  2263942.8\n",
      "-배추 가격:  [2578.9685]\n",
      "# 33500  손실비용:  2263664.2\n",
      "-배추 가격:  [2579.16]\n",
      "# 34000  손실비용:  2263386.5\n",
      "-배추 가격:  [2579.2925]\n",
      "# 34500  손실비용:  2263109.5\n",
      "-배추 가격:  [2579.4211]\n",
      "# 35000  손실비용:  2262832.5\n",
      "-배추 가격:  [2579.5493]\n",
      "# 35500  손실비용:  2262555.8\n",
      "-배추 가격:  [2579.677]\n",
      "# 36000  손실비용:  2262279.0\n",
      "-배추 가격:  [2579.8047]\n",
      "# 36500  손실비용:  2262002.2\n",
      "-배추 가격:  [2579.9326]\n",
      "# 37000  손실비용:  2261725.8\n",
      "-배추 가격:  [2580.0603]\n",
      "# 37500  손실비용:  2261449.5\n",
      "-배추 가격:  [2580.1853]\n",
      "# 38000  손실비용:  2261173.2\n",
      "-배추 가격:  [2580.3054]\n",
      "# 38500  손실비용:  2260897.0\n",
      "-배추 가격:  [2580.3652]\n",
      "# 39000  손실비용:  2260621.0\n",
      "-배추 가격:  [2580.4026]\n",
      "# 39500  손실비용:  2260345.0\n",
      "-배추 가격:  [2580.44]\n",
      "# 40000  손실비용:  2260070.5\n",
      "-배추 가격:  [2580.4763]\n",
      "# 40500  손실비용:  2259795.5\n",
      "-배추 가격:  [2580.5112]\n",
      "# 41000  손실비용:  2259521.2\n",
      "-배추 가격:  [2580.544]\n",
      "# 41500  손실비용:  2259246.5\n",
      "-배추 가격:  [2580.5767]\n",
      "# 42000  손실비용:  2258972.8\n",
      "-배추 가격:  [2580.6099]\n",
      "# 42500  손실비용:  2258698.5\n",
      "-배추 가격:  [2580.6433]\n",
      "# 43000  손실비용:  2258424.2\n",
      "-배추 가격:  [2580.6768]\n",
      "# 43500  손실비용:  2258150.5\n",
      "-배추 가격:  [2580.71]\n",
      "# 44000  손실비용:  2257876.8\n",
      "-배추 가격:  [2580.7427]\n",
      "# 44500  손실비용:  2257603.2\n",
      "-배추 가격:  [2580.7766]\n",
      "# 45000  손실비용:  2257329.8\n",
      "-배추 가격:  [2580.81]\n",
      "# 45500  손실비용:  2257056.5\n",
      "-배추 가격:  [2580.8435]\n",
      "# 46000  손실비용:  2256783.8\n",
      "-배추 가격:  [2580.8774]\n",
      "# 46500  손실비용:  2256510.0\n",
      "-배추 가격:  [2580.9124]\n",
      "# 47000  손실비용:  2256237.5\n",
      "-배추 가격:  [2580.9485]\n",
      "# 47500  손실비용:  2255965.5\n",
      "-배추 가격:  [2580.9827]\n",
      "# 48000  손실비용:  2255694.2\n",
      "-배추 가격:  [2581.0168]\n",
      "# 48500  손실비용:  2255423.0\n",
      "-배추 가격:  [2581.0532]\n",
      "# 49000  손실비용:  2255151.5\n",
      "-배추 가격:  [2581.0918]\n",
      "# 49500  손실비용:  2254880.5\n",
      "-배추 가격:  [2581.131]\n",
      "# 50000  손실비용:  2254609.8\n",
      "-배추 가격:  [2581.1733]\n",
      "# 50500  손실비용:  2254339.2\n",
      "-배추 가격:  [2581.215]\n",
      "# 51000  손실비용:  2254068.2\n",
      "-배추 가격:  [2581.2585]\n",
      "# 51500  손실비용:  2253797.2\n",
      "-배추 가격:  [2581.3047]\n",
      "# 52000  손실비용:  2253527.2\n",
      "-배추 가격:  [2581.3508]\n",
      "# 52500  손실비용:  2253257.2\n",
      "-배추 가격:  [2581.3965]\n",
      "# 53000  손실비용:  2252986.8\n",
      "-배추 가격:  [2581.4434]\n",
      "# 53500  손실비용:  2252716.8\n",
      "-배추 가격:  [2581.4932]\n",
      "# 54000  손실비용:  2252446.5\n",
      "-배추 가격:  [2581.5432]\n",
      "# 54500  손실비용:  2252177.0\n",
      "-배추 가격:  [2581.592]\n",
      "# 55000  손실비용:  2251908.8\n",
      "-배추 가격:  [2581.6367]\n",
      "# 55500  손실비용:  2251640.2\n",
      "-배추 가격:  [2581.6816]\n",
      "# 56000  손실비용:  2251371.8\n",
      "-배추 가격:  [2581.7297]\n",
      "# 56500  손실비용:  2251103.5\n",
      "-배추 가격:  [2581.7778]\n",
      "# 57000  손실비용:  2250835.5\n",
      "-배추 가격:  [2581.826]\n",
      "# 57500  손실비용:  2250567.2\n",
      "-배추 가격:  [2581.8743]\n",
      "# 58000  손실비용:  2250299.5\n",
      "-배추 가격:  [2581.9226]\n",
      "# 58500  손실비용:  2250032.2\n",
      "-배추 가격:  [2581.9675]\n",
      "# 59000  손실비용:  2249765.8\n",
      "-배추 가격:  [2582.0103]\n",
      "# 59500  손실비용:  2249499.2\n",
      "-배추 가격:  [2582.0603]\n",
      "# 60000  손실비용:  2249233.2\n",
      "-배추 가격:  [2582.1108]\n",
      "# 60500  손실비용:  2248966.8\n",
      "-배추 가격:  [2582.161]\n",
      "# 61000  손실비용:  2248701.2\n",
      "-배추 가격:  [2582.2017]\n",
      "# 61500  손실비용:  2248434.8\n",
      "-배추 가격:  [2582.2349]\n",
      "# 62000  손실비용:  2248169.0\n",
      "-배추 가격:  [2582.2683]\n",
      "# 62500  손실비용:  2247903.2\n",
      "-배추 가격:  [2582.3015]\n",
      "# 63000  손실비용:  2247637.8\n",
      "-배추 가격:  [2582.3354]\n",
      "# 63500  손실비용:  2247372.0\n",
      "-배추 가격:  [2582.3687]\n",
      "# 64000  손실비용:  2247106.8\n",
      "-배추 가격:  [2582.402]\n",
      "# 64500  손실비용:  2246841.5\n",
      "-배추 가격:  [2582.4358]\n",
      "# 65000  손실비용:  2246576.2\n",
      "-배추 가격:  [2582.4697]\n",
      "# 65500  손실비용:  2246311.5\n",
      "-배추 가격:  [2582.5037]\n",
      "# 66000  손실비용:  2246046.8\n",
      "-배추 가격:  [2582.5469]\n",
      "# 66500  손실비용:  2245781.8\n",
      "-배추 가격:  [2582.5981]\n",
      "# 67000  손실비용:  2245517.0\n",
      "-배추 가격:  [2582.6506]\n",
      "# 67500  손실비용:  2245252.0\n",
      "-배추 가격:  [2582.7024]\n",
      "# 68000  손실비용:  2244987.5\n",
      "-배추 가격:  [2582.7532]\n",
      "# 68500  손실비용:  2244723.0\n",
      "-배추 가격:  [2582.804]\n",
      "# 69000  손실비용:  2244459.0\n",
      "-배추 가격:  [2582.849]\n",
      "# 69500  손실비용:  2244195.0\n",
      "-배추 가격:  [2582.8945]\n",
      "# 70000  손실비용:  2243930.8\n",
      "-배추 가격:  [2582.9395]\n",
      "# 70500  손실비용:  2243666.5\n",
      "-배추 가격:  [2582.9795]\n",
      "# 71000  손실비용:  2243403.0\n",
      "-배추 가격:  [2583.0195]\n",
      "# 71500  손실비용:  2243139.2\n",
      "-배추 가격:  [2583.0596]\n",
      "# 72000  손실비용:  2242875.8\n",
      "-배추 가격:  [2583.0952]\n",
      "# 72500  손실비용:  2242612.5\n",
      "-배추 가격:  [2583.13]\n",
      "# 73000  손실비용:  2242349.2\n",
      "-배추 가격:  [2583.1638]\n",
      "# 73500  손실비용:  2242088.2\n",
      "-배추 가격:  [2583.191]\n",
      "# 74000  손실비용:  2241827.5\n",
      "-배추 가격:  [2583.214]\n",
      "# 74500  손실비용:  2241567.0\n",
      "-배추 가격:  [2583.2363]\n",
      "# 75000  손실비용:  2241306.8\n",
      "-배추 가격:  [2583.2583]\n",
      "# 75500  손실비용:  2241046.0\n",
      "-배추 가격:  [2583.28]\n",
      "# 76000  손실비용:  2240785.8\n",
      "-배추 가격:  [2583.3025]\n",
      "# 76500  손실비용:  2240525.8\n",
      "-배추 가격:  [2583.3228]\n",
      "# 77000  손실비용:  2240265.8\n",
      "-배추 가격:  [2583.3398]\n",
      "# 77500  손실비용:  2240005.5\n",
      "-배추 가격:  [2583.3564]\n",
      "# 78000  손실비용:  2239745.8\n",
      "-배추 가격:  [2583.3733]\n",
      "# 78500  손실비용:  2239485.8\n",
      "-배추 가격:  [2583.39]\n",
      "# 79000  손실비용:  2239226.0\n",
      "-배추 가격:  [2583.4067]\n",
      "# 79500  손실비용:  2238966.5\n",
      "-배추 가격:  [2583.4236]\n",
      "# 80000  손실비용:  2238707.5\n",
      "-배추 가격:  [2583.439]\n",
      "# 80500  손실비용:  2238447.5\n",
      "-배추 가격:  [2583.4507]\n",
      "# 81000  손실비용:  2238188.5\n",
      "-배추 가격:  [2583.4622]\n",
      "# 81500  손실비용:  2237929.5\n",
      "-배추 가격:  [2583.4734]\n",
      "# 82000  손실비용:  2237670.5\n",
      "-배추 가격:  [2583.485]\n",
      "# 82500  손실비용:  2237411.5\n",
      "-배추 가격:  [2583.4963]\n",
      "# 83000  손실비용:  2237152.8\n",
      "-배추 가격:  [2583.508]\n",
      "# 83500  손실비용:  2236894.0\n",
      "-배추 가격:  [2583.5193]\n",
      "# 84000  손실비용:  2236635.2\n",
      "-배추 가격:  [2583.5308]\n",
      "# 84500  손실비용:  2236376.5\n",
      "-배추 가격:  [2583.5425]\n",
      "# 85000  손실비용:  2236117.8\n",
      "-배추 가격:  [2583.5515]\n",
      "# 85500  손실비용:  2235859.8\n",
      "-배추 가격:  [2583.5576]\n",
      "# 86000  손실비용:  2235601.8\n",
      "-배추 가격:  [2583.5637]\n",
      "# 86500  손실비용:  2235343.5\n",
      "-배추 가격:  [2583.5698]\n",
      "# 87000  손실비용:  2235085.2\n",
      "-배추 가격:  [2583.5762]\n",
      "# 87500  손실비용:  2234827.5\n",
      "-배추 가격:  [2583.582]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# 88000  손실비용:  2234569.8\n",
      "-배추 가격:  [2583.6074]\n",
      "# 88500  손실비용:  2234312.2\n",
      "-배추 가격:  [2583.6416]\n",
      "# 89000  손실비용:  2234055.0\n",
      "-배추 가격:  [2583.6758]\n",
      "# 89500  손실비용:  2233797.8\n",
      "-배추 가격:  [2583.7102]\n",
      "# 90000  손실비용:  2233540.5\n",
      "-배추 가격:  [2583.7446]\n",
      "# 90500  손실비용:  2233285.0\n",
      "-배추 가격:  [2583.7786]\n",
      "# 91000  손실비용:  2233030.2\n",
      "-배추 가격:  [2583.8123]\n",
      "# 91500  손실비용:  2232775.8\n",
      "-배추 가격:  [2583.8455]\n",
      "# 92000  손실비용:  2232521.8\n",
      "-배추 가격:  [2583.8796]\n",
      "# 92500  손실비용:  2232267.5\n",
      "-배추 가격:  [2583.9136]\n",
      "# 93000  손실비용:  2232013.2\n",
      "-배추 가격:  [2583.9478]\n",
      "# 93500  손실비용:  2231759.0\n",
      "-배추 가격:  [2583.9814]\n",
      "# 94000  손실비용:  2231505.2\n",
      "-배추 가격:  [2584.0156]\n",
      "# 94500  손실비용:  2231251.2\n",
      "-배추 가격:  [2584.0498]\n",
      "# 95000  손실비용:  2230997.8\n",
      "-배추 가격:  [2584.0837]\n",
      "# 95500  손실비용:  2230744.2\n",
      "-배추 가격:  [2584.1182]\n",
      "# 96000  손실비용:  2230490.8\n",
      "-배추 가격:  [2584.152]\n",
      "# 96500  손실비용:  2230237.5\n",
      "-배추 가격:  [2584.186]\n",
      "# 97000  손실비용:  2229984.8\n",
      "-배추 가격:  [2584.2207]\n",
      "# 97500  손실비용:  2229731.5\n",
      "-배추 가격:  [2584.2559]\n",
      "# 98000  손실비용:  2229478.5\n",
      "-배추 가격:  [2584.2898]\n",
      "# 98500  손실비용:  2229225.5\n",
      "-배추 가격:  [2584.3247]\n",
      "# 99000  손실비용:  2228972.8\n",
      "-배추 가격:  [2584.3591]\n",
      "# 99500  손실비용:  2228720.0\n",
      "-배추 가격:  [2584.3936]\n"
     ]
    }
   ],
   "source": [
    "# 학습을 수행한다.\n",
    "ephocs = 100000\n",
    "for step in range(ephocs):\n",
    "    costx, hypox, trainx = sess.run([cost, hypothesis, train], feed_dict={X: x_data, Y: y_data})\n",
    "    if step % 500 == 0:\n",
    "        print(\"#\", step, \" 손실비용: \", costx)\n",
    "        print(\"-배추 가격: \", hypox[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "학습된 모델을 저장했습니다.\n"
     ]
    }
   ],
   "source": [
    "# 학습된 모델을 저장한다.\n",
    "saver = tf.train.Saver()\n",
    "save_path = saver.save(sess, \"./saved.cpkt\") #tensorflow로 학습시킨 딥러닝 모델을 저장하는 방법, 학습된 모델의 variable을 저장.\n",
    "print(\"학습된 모델을 저장했습니다.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 학습모델을 저장하는 이유\n",
    "\n",
    "만약 학습 모델을 저장하지 않고 사용자로부터 배추 가격 예측 요청(Request)이 발생할 때마다 매 번 학습을 해서 결과를 돌려준다면 시간이 오래 걸리고 많은 리소스가 소모 될 것이다.\n",
    "\n",
    "  따라서 오프라인(Offline)에서 주기적으로 데이터를 이용해 학습을 진행한 뒤에, 그 결과를 저장해서 서버에서는 실시간 요청에 따라 이미 저장된 학습 모델로 배추 가격을 예측하는 것이 효율적이다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# 0  손실비용:  12288999.0\n",
      "-배추 가격:  [-11.698873]\n",
      "# 1000  손실비용:  3787046.8\n",
      "-배추 가격:  [33.47626]\n",
      "# 2000  손실비용:  3219893.2\n",
      "-배추 가격:  [566.59204]\n",
      "# 3000  손실비용:  2867706.0\n",
      "-배추 가격:  [987.4247]\n",
      "# 4000  손실비용:  2647718.8\n",
      "-배추 가격:  [1319.9]\n",
      "# 5000  손실비용:  2509920.0\n",
      "-배추 가격:  [1582.7433]\n",
      "# 6000  손실비용:  2423446.5\n",
      "-배추 가격:  [1790.624]\n",
      "# 7000  손실비용:  2369081.5\n",
      "-배추 가격:  [1955.0833]\n",
      "# 8000  손실비용:  2334818.0\n",
      "-배추 가격:  [2085.213]\n",
      "# 9000  손실비용:  2313143.8\n",
      "-배추 가격:  [2188.193]\n",
      "# 10000  손실비용:  2299353.5\n",
      "-배추 가격:  [2269.6963]\n",
      "# 11000  손실비용:  2290502.0\n",
      "-배추 가격:  [2334.205]\n",
      "# 12000  손실비용:  2284744.2\n",
      "-배추 가격:  [2385.2683]\n",
      "# 13000  손실비용:  2280924.5\n",
      "-배추 가격:  [2425.69]\n",
      "# 14000  손실비용:  2278319.0\n",
      "-배추 가격:  [2457.6907]\n",
      "# 15000  손실비용:  2276474.0\n",
      "-배추 가격:  [2483.0276]\n",
      "# 16000  손실비용:  2275106.2\n",
      "-배추 가격:  [2503.091]\n",
      "# 17000  손실비용:  2274036.8\n",
      "-배추 가격:  [2518.9805]\n",
      "# 18000  손실비용:  2273155.5\n",
      "-배추 가격:  [2531.5657]\n",
      "# 19000  손실비용:  2272391.8\n",
      "-배추 가격:  [2541.537]\n",
      "# 20000  손실비용:  2271702.0\n",
      "-배추 가격:  [2549.4377]\n",
      "# 21000  손실비용:  2271059.5\n",
      "-배추 가격:  [2555.7021]\n",
      "# 22000  손실비용:  2270445.8\n",
      "-배추 가격:  [2560.6682]\n",
      "# 23000  손실비용:  2269851.5\n",
      "-배추 가격:  [2564.6113]\n",
      "# 24000  손실비용:  2269269.2\n",
      "-배추 가격:  [2567.7354]\n",
      "# 25000  손실비용:  2268694.5\n",
      "-배추 가격:  [2570.2212]\n",
      "# 26000  손실비용:  2268125.2\n",
      "-배추 가격:  [2572.1973]\n",
      "# 27000  손실비용:  2267559.8\n",
      "-배추 가격:  [2573.7712]\n",
      "# 28000  손실비용:  2266996.2\n",
      "-배추 가격:  [2575.0261]\n",
      "# 29000  손실비용:  2266435.5\n",
      "-배추 가격:  [2576.0444]\n",
      "# 30000  손실비용:  2265875.5\n",
      "-배추 가격:  [2576.856]\n",
      "# 31000  손실비용:  2265317.0\n",
      "-배추 가격:  [2577.5127]\n",
      "# 32000  손실비용:  2264759.8\n",
      "-배추 가격:  [2578.0059]\n",
      "# 33000  손실비용:  2264202.8\n",
      "-배추 가격:  [2578.4707]\n",
      "# 34000  손실비용:  2263646.2\n",
      "-배추 가격:  [2578.7979]\n",
      "# 35000  손실비용:  2263092.0\n",
      "-배추 가격:  [2579.0552]\n",
      "# 36000  손실비용:  2262538.8\n",
      "-배추 가격:  [2579.3105]\n",
      "# 37000  손실비용:  2261985.5\n",
      "-배추 가격:  [2579.5632]\n",
      "# 38000  손실비용:  2261433.2\n",
      "-배추 가격:  [2579.7993]\n",
      "# 39000  손실비용:  2260881.5\n",
      "-배추 가격:  [2579.917]\n",
      "# 40000  손실비용:  2260329.8\n",
      "-배추 가격:  [2579.9922]\n",
      "# 41000  손실비용:  2259780.8\n",
      "-배추 가격:  [2580.0608]\n",
      "# 42000  손실비용:  2259232.5\n",
      "-배추 가격:  [2580.1277]\n",
      "# 43000  손실비용:  2258684.2\n",
      "-배추 가격:  [2580.1943]\n",
      "# 44000  손실비용:  2258137.0\n",
      "-배추 가격:  [2580.2615]\n",
      "# 45000  손실비용:  2257590.5\n",
      "-배추 가격:  [2580.3315]\n",
      "# 46000  손실비용:  2257044.2\n",
      "-배추 가격:  [2580.4077]\n",
      "# 47000  손실비용:  2256498.5\n",
      "-배추 가격:  [2580.4895]\n",
      "# 48000  손실비용:  2255954.5\n",
      "-배추 가격:  [2580.572]\n",
      "# 49000  손실비용:  2255412.2\n",
      "-배추 가격:  [2580.6548]\n",
      "# 50000  손실비용:  2254870.5\n",
      "-배추 가격:  [2580.742]\n",
      "# 51000  손실비용:  2254329.0\n",
      "-배추 가격:  [2580.834]\n",
      "# 52000  손실비용:  2253788.2\n",
      "-배추 가격:  [2580.928]\n",
      "# 53000  손실비용:  2253247.8\n",
      "-배추 가격:  [2581.025]\n",
      "# 54000  손실비용:  2252707.8\n",
      "-배추 가격:  [2581.1306]\n",
      "# 55000  손실비용:  2252169.0\n",
      "-배추 가격:  [2581.2346]\n",
      "# 56000  손실비용:  2251632.2\n",
      "-배추 가격:  [2581.331]\n",
      "# 57000  손실비용:  2251096.5\n",
      "-배추 가격:  [2581.4272]\n",
      "# 58000  손실비용:  2250560.2\n",
      "-배추 가격:  [2581.5237]\n",
      "# 59000  손실비용:  2250024.8\n",
      "-배추 가격:  [2581.6199]\n",
      "# 60000  손실비용:  2249491.5\n",
      "-배추 가격:  [2581.7156]\n",
      "# 61000  손실비용:  2248959.5\n",
      "-배추 가격:  [2581.8164]\n",
      "# 62000  손실비용:  2248427.8\n",
      "-배추 가격:  [2581.9175]\n",
      "# 63000  손실비용:  2247896.5\n",
      "-배추 가격:  [2582.017]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-7-2ceb0b533fd9>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     45\u001b[0m     \u001b[0mephocs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m100000\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     46\u001b[0m     \u001b[1;32mfor\u001b[0m \u001b[0mstep\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mephocs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 47\u001b[1;33m         \u001b[0mcostx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhypox\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtrainx\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0msess\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mcost\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhypothesis\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtrain\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m{\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mx_data\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mY\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0my_data\u001b[0m\u001b[1;33m}\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     48\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mstep\u001b[0m \u001b[1;33m%\u001b[0m \u001b[1;36m1000\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     49\u001b[0m             \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"#\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mstep\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\" 손실비용: \"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcostx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python38\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36mrun\u001b[1;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[0;32m    955\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    956\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 957\u001b[1;33m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[0m\u001b[0;32m    958\u001b[0m                          run_metadata_ptr)\n\u001b[0;32m    959\u001b[0m       \u001b[1;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python38\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_run\u001b[1;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[0;32m   1178\u001b[0m     \u001b[1;31m# or if the call is a partial run that specifies feeds.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1179\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mfinal_fetches\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0mfinal_targets\u001b[0m \u001b[1;32mor\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mhandle\u001b[0m \u001b[1;32mand\u001b[0m \u001b[0mfeed_dict_tensor\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1180\u001b[1;33m       results = self._do_run(handle, final_targets, final_fetches,\n\u001b[0m\u001b[0;32m   1181\u001b[0m                              feed_dict_tensor, options, run_metadata)\n\u001b[0;32m   1182\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python38\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_do_run\u001b[1;34m(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\u001b[0m\n\u001b[0;32m   1356\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1357\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mhandle\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1358\u001b[1;33m       return self._do_call(_run_fn, feeds, fetches, targets, options,\n\u001b[0m\u001b[0;32m   1359\u001b[0m                            run_metadata)\n\u001b[0;32m   1360\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python38\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_do_call\u001b[1;34m(self, fn, *args)\u001b[0m\n\u001b[0;32m   1363\u001b[0m   \u001b[1;32mdef\u001b[0m \u001b[0m_do_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1364\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1365\u001b[1;33m       \u001b[1;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1366\u001b[0m     \u001b[1;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mOpError\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1367\u001b[0m       \u001b[0mmessage\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcompat\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mas_text\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0me\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmessage\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python38\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_run_fn\u001b[1;34m(feed_dict, fetch_list, target_list, options, run_metadata)\u001b[0m\n\u001b[0;32m   1347\u001b[0m       \u001b[1;31m# Ensure any changes to the graph are reflected in the runtime.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1348\u001b[0m       \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_extend_graph\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1349\u001b[1;33m       return self._call_tf_sessionrun(options, feed_dict, fetch_list,\n\u001b[0m\u001b[0;32m   1350\u001b[0m                                       target_list, run_metadata)\n\u001b[0;32m   1351\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python38\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_call_tf_sessionrun\u001b[1;34m(self, options, feed_dict, fetch_list, target_list, run_metadata)\u001b[0m\n\u001b[0;32m   1439\u001b[0m   def _call_tf_sessionrun(self, options, feed_dict, fetch_list, target_list,\n\u001b[0;32m   1440\u001b[0m                           run_metadata):\n\u001b[1;32m-> 1441\u001b[1;33m     return tf_session.TF_SessionRun_wrapper(self._session, options, feed_dict,\n\u001b[0m\u001b[0;32m   1442\u001b[0m                                             \u001b[0mfetch_list\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtarget_list\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1443\u001b[0m                                             run_metadata)\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Tensorflow Ver1.\n",
    "# 엑셀에서 data를 읽어온다.\n",
    "# pandas library: excel data를 읽어올 수 있다.\n",
    "import tensorflow.compat.v1 as tf\n",
    "import numpy as np\n",
    "from pandas.io.parsers import read_csv\n",
    "tf.disable_v2_behavior()\n",
    "\n",
    "data = read_csv('price data.csv', sep=',')  # csv파일 읽어오기\n",
    "\n",
    "xy = np.array(data, dtype=np.float32)    # 행렬 형태로 저장하기\n",
    "\n",
    "# 4개의 변인을 입력을 받는다.\n",
    "x_data = xy[:, 1:-1]   # slicing x\n",
    "\n",
    "# 가격 값을 입력 받는다.\n",
    "y_data = xy[:, [-1]]   # slicing y\n",
    "\n",
    "# placeholder를 설정한다.\n",
    "# Tensorflow2에선 생략\n",
    "X = tf.placeholder(tf.float32, shape=[None, 4])  # 변인 4개\n",
    "Y = tf.placeholder(tf.float32, shape=[None, 1])  # 변인 1개\n",
    "W = tf.Variable(tf.random_normal([4, 1]), name=\"weight\")\n",
    "b = tf.Variable(tf.random_normal([1]), name=\"bias\")\n",
    "\n",
    "# 가설을 설정한다.\n",
    "hypothesis = tf.matmul(X, W) + b  # 행렬 곱 연산\n",
    "\n",
    "# 비용함수를 설정한다.\n",
    "# mse(mean squared error)\n",
    "cost = tf.reduce_mean(tf.square(hypothesis - Y))\n",
    "\n",
    "# 최적화 함수를 설정한다. 여기선 GradientDescent(경사하강) 함수 사용\n",
    "# 학습률 0.000005로 설정한 이유 - 되도록 짧은 시간에 정확한 결과 나오게 하기 위함\n",
    "# 경사하강의 간격을 의미함\n",
    "optimizer = tf.train.GradientDescentOptimizer(learning_rate=0.000005)\n",
    "train = optimizer.minimize(cost)\n",
    "\n",
    "# 세션을 생성한다.\n",
    "with tf.Session() as sess:\n",
    "    # 글로벌 변수를 초기화해준다.\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    \n",
    "    # 학습을 수행한다.\n",
    "    ephocs = 100000\n",
    "    for step in range(ephocs):\n",
    "        costx, hypox, trainx = sess.run([cost, hypothesis, train], feed_dict={X: x_data, Y: y_data})\n",
    "        if step % 1000 == 0:\n",
    "            print(\"#\", step, \" 손실비용: \", costx)\n",
    "            print(\"-배추 가격: \", hypox[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_1 (Dense)              (None, 1)                 5         \n",
      "=================================================================\n",
      "Total params: 5\n",
      "Trainable params: 5\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 2922 samples\n",
      "Epoch 1/300\n",
      "2922/2922 [==============================] - 0s 5us/sample - loss: 11189315.7105\n",
      "Epoch 2/300\n",
      "2922/2922 [==============================] - 0s 5us/sample - loss: 9027273.9062\n",
      "Epoch 3/300\n",
      "2922/2922 [==============================] - 0s 6us/sample - loss: 7567151.9151\n",
      "Epoch 4/300\n",
      "2922/2922 [==============================] - 0s 5us/sample - loss: 6579780.3364\n",
      "Epoch 5/300\n",
      "2922/2922 [==============================] - 0s 5us/sample - loss: 5908905.0640\n",
      "Epoch 6/300\n",
      "2922/2922 [==============================] - 0s 5us/sample - loss: 5451557.0188\n",
      "Epoch 7/300\n",
      "2922/2922 [==============================] - 0s 5us/sample - loss: 5136222.9644\n",
      "Epoch 8/300\n",
      "2922/2922 [==============================] - 0s 5us/sample - loss: 4916680.7560\n",
      "Epoch 9/300\n",
      "2922/2922 [==============================] - 0s 5us/sample - loss: 4761677.8463\n",
      "Epoch 10/300\n",
      "2922/2922 [==============================] - 0s 5us/sample - loss: 4650039.1697\n",
      "Epoch 11/300\n",
      "2922/2922 [==============================] - 0s 5us/sample - loss: 4567392.6872\n",
      "Epoch 12/300\n",
      "2922/2922 [==============================] - 0s 5us/sample - loss: 4504489.1157\n",
      "Epoch 13/300\n",
      "2922/2922 [==============================] - 0s 6us/sample - loss: 4454629.1246\n",
      "Epoch 14/300\n",
      "2922/2922 [==============================] - 0s 5us/sample - loss: 4413650.7717\n",
      "Epoch 15/300\n",
      "2922/2922 [==============================] - 0s 6us/sample - loss: 4378674.9456\n",
      "Epoch 16/300\n",
      "2922/2922 [==============================] - 0s 5us/sample - loss: 4347945.7837\n",
      "Epoch 17/300\n",
      "2922/2922 [==============================] - 0s 5us/sample - loss: 4320064.8285\n",
      "Epoch 18/300\n",
      "2922/2922 [==============================] - 0s 5us/sample - loss: 4294380.9221\n",
      "Epoch 19/300\n",
      "2922/2922 [==============================] - 0s 5us/sample - loss: 4269891.4921\n",
      "Epoch 20/300\n",
      "2922/2922 [==============================] - 0s 5us/sample - loss: 4246619.6751\n",
      "Epoch 21/300\n",
      "2922/2922 [==============================] - 0s 5us/sample - loss: 4224011.2601\n",
      "Epoch 22/300\n",
      "2922/2922 [==============================] - 0s 5us/sample - loss: 4202119.7998\n",
      "Epoch 23/300\n",
      "2922/2922 [==============================] - 0s 5us/sample - loss: 4180507.7575\n",
      "Epoch 24/300\n",
      "2922/2922 [==============================] - 0s 6us/sample - loss: 4159493.0780\n",
      "Epoch 25/300\n",
      "2922/2922 [==============================] - 0s 5us/sample - loss: 4138698.4894\n",
      "Epoch 26/300\n",
      "2922/2922 [==============================] - 0s 5us/sample - loss: 4118320.9403\n",
      "Epoch 27/300\n",
      "2922/2922 [==============================] - 0s 5us/sample - loss: 4098144.8871\n",
      "Epoch 28/300\n",
      "2922/2922 [==============================] - 0s 5us/sample - loss: 4078423.9247\n",
      "Epoch 29/300\n",
      "2922/2922 [==============================] - 0s 5us/sample - loss: 4058640.0221\n",
      "Epoch 30/300\n",
      "2922/2922 [==============================] - 0s 5us/sample - loss: 4039351.4021\n",
      "Epoch 31/300\n",
      "2922/2922 [==============================] - 0s 5us/sample - loss: 4020124.7957\n",
      "Epoch 32/300\n",
      "2922/2922 [==============================] - 0s 6us/sample - loss: 4001137.6585\n",
      "Epoch 33/300\n",
      "2922/2922 [==============================] - 0s 5us/sample - loss: 3982352.1253\n",
      "Epoch 34/300\n",
      "2922/2922 [==============================] - 0s 5us/sample - loss: 3963766.6838\n",
      "Epoch 35/300\n",
      "2922/2922 [==============================] - 0s 5us/sample - loss: 3945496.6550\n",
      "Epoch 36/300\n",
      "2922/2922 [==============================] - 0s 5us/sample - loss: 3927324.4813\n",
      "Epoch 37/300\n",
      "2922/2922 [==============================] - 0s 5us/sample - loss: 3909341.6256\n",
      "Epoch 38/300\n",
      "2922/2922 [==============================] - 0s 5us/sample - loss: 3891646.3439\n",
      "Epoch 39/300\n",
      "2922/2922 [==============================] - 0s 5us/sample - loss: 3874204.9097\n",
      "Epoch 40/300\n",
      "2922/2922 [==============================] - 0s 5us/sample - loss: 3856955.6713\n",
      "Epoch 41/300\n",
      "2922/2922 [==============================] - 0s 5us/sample - loss: 3839794.2579\n",
      "Epoch 42/300\n",
      "2922/2922 [==============================] - 0s 5us/sample - loss: 3822865.8032\n",
      "Epoch 43/300\n",
      "2922/2922 [==============================] - 0s 4us/sample - loss: 3806010.0388\n",
      "Epoch 44/300\n",
      "2922/2922 [==============================] - 0s 4us/sample - loss: 3789526.4028\n",
      "Epoch 45/300\n",
      "2922/2922 [==============================] - 0s 5us/sample - loss: 3773149.2132\n",
      "Epoch 46/300\n",
      "2922/2922 [==============================] - 0s 5us/sample - loss: 3756933.6614\n",
      "Epoch 47/300\n",
      "2922/2922 [==============================] - 0s 5us/sample - loss: 3740920.5513\n",
      "Epoch 48/300\n",
      "2922/2922 [==============================] - 0s 4us/sample - loss: 3725062.9974\n",
      "Epoch 49/300\n",
      "2922/2922 [==============================] - 0s 5us/sample - loss: 3709410.5585\n",
      "Epoch 50/300\n",
      "2922/2922 [==============================] - 0s 5us/sample - loss: 3694000.4475\n",
      "Epoch 51/300\n",
      "2922/2922 [==============================] - 0s 5us/sample - loss: 3678722.0727\n",
      "Epoch 52/300\n",
      "2922/2922 [==============================] - 0s 5us/sample - loss: 3663491.4511\n",
      "Epoch 53/300\n",
      "2922/2922 [==============================] - 0s 4us/sample - loss: 3648547.6910\n",
      "Epoch 54/300\n",
      "2922/2922 [==============================] - 0s 5us/sample - loss: 3633816.4894\n",
      "Epoch 55/300\n",
      "2922/2922 [==============================] - 0s 5us/sample - loss: 3619005.1299\n",
      "Epoch 56/300\n",
      "2922/2922 [==============================] - 0s 6us/sample - loss: 3604756.8154\n",
      "Epoch 57/300\n",
      "2922/2922 [==============================] - 0s 5us/sample - loss: 3590206.7146\n",
      "Epoch 58/300\n",
      "2922/2922 [==============================] - 0s 5us/sample - loss: 3576007.9976\n",
      "Epoch 59/300\n",
      "2922/2922 [==============================] - 0s 5us/sample - loss: 3562042.4004\n",
      "Epoch 60/300\n",
      "2922/2922 [==============================] - 0s 5us/sample - loss: 3548216.2930\n",
      "Epoch 61/300\n",
      "2922/2922 [==============================] - 0s 5us/sample - loss: 3534464.6051\n",
      "Epoch 62/300\n",
      "2922/2922 [==============================] - 0s 5us/sample - loss: 3520938.5763\n",
      "Epoch 63/300\n",
      "2922/2922 [==============================] - 0s 5us/sample - loss: 3507590.7651\n",
      "Epoch 64/300\n",
      "2922/2922 [==============================] - 0s 6us/sample - loss: 3494235.0171\n",
      "Epoch 65/300\n",
      "2922/2922 [==============================] - 0s 5us/sample - loss: 3481189.4281\n",
      "Epoch 66/300\n",
      "2922/2922 [==============================] - 0s 5us/sample - loss: 3468221.4507\n",
      "Epoch 67/300\n",
      "2922/2922 [==============================] - 0s 5us/sample - loss: 3455325.8284\n",
      "Epoch 68/300\n",
      "2922/2922 [==============================] - 0s 4us/sample - loss: 3442665.4697\n",
      "Epoch 69/300\n",
      "2922/2922 [==============================] - 0s 4us/sample - loss: 3430121.7207\n",
      "Epoch 70/300\n",
      "2922/2922 [==============================] - 0s 5us/sample - loss: 3417703.8939\n",
      "Epoch 71/300\n",
      "2922/2922 [==============================] - 0s 5us/sample - loss: 3405427.4394\n",
      "Epoch 72/300\n",
      "2922/2922 [==============================] - 0s 5us/sample - loss: 3393332.7996\n",
      "Epoch 73/300\n",
      "2922/2922 [==============================] - 0s 5us/sample - loss: 3381395.8412\n",
      "Epoch 74/300\n",
      "2922/2922 [==============================] - 0s 8us/sample - loss: 3369445.0914\n",
      "Epoch 75/300\n",
      "2922/2922 [==============================] - 0s 7us/sample - loss: 3357776.0686\n",
      "Epoch 76/300\n",
      "2922/2922 [==============================] - 0s 5us/sample - loss: 3346139.2562\n",
      "Epoch 77/300\n",
      "2922/2922 [==============================] - 0s 5us/sample - loss: 3334574.0927\n",
      "Epoch 78/300\n",
      "2922/2922 [==============================] - 0s 5us/sample - loss: 3323200.9767\n",
      "Epoch 79/300\n",
      "2922/2922 [==============================] - 0s 5us/sample - loss: 3312035.5041\n",
      "Epoch 80/300\n",
      "2922/2922 [==============================] - 0s 5us/sample - loss: 3300896.2325\n",
      "Epoch 81/300\n",
      "2922/2922 [==============================] - 0s 5us/sample - loss: 3289885.2207\n",
      "Epoch 82/300\n",
      "2922/2922 [==============================] - 0s 5us/sample - loss: 3278953.1778\n",
      "Epoch 83/300\n",
      "2922/2922 [==============================] - 0s 5us/sample - loss: 3268199.7118\n",
      "Epoch 84/300\n",
      "2922/2922 [==============================] - 0s 5us/sample - loss: 3257560.4026\n",
      "Epoch 85/300\n",
      "2922/2922 [==============================] - 0s 4us/sample - loss: 3247035.4509\n",
      "Epoch 86/300\n",
      "2922/2922 [==============================] - 0s 4us/sample - loss: 3236667.1819\n",
      "Epoch 87/300\n",
      "2922/2922 [==============================] - 0s 4us/sample - loss: 3226484.9257\n",
      "Epoch 88/300\n",
      "2922/2922 [==============================] - 0s 4us/sample - loss: 3216186.0789\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 89/300\n",
      "2922/2922 [==============================] - 0s 4us/sample - loss: 3206172.1932\n",
      "Epoch 90/300\n",
      "2922/2922 [==============================] - 0s 4us/sample - loss: 3196158.9721\n",
      "Epoch 91/300\n",
      "2922/2922 [==============================] - 0s 4us/sample - loss: 3186280.6804\n",
      "Epoch 92/300\n",
      "2922/2922 [==============================] - 0s 4us/sample - loss: 3176655.5866\n",
      "Epoch 93/300\n",
      "2922/2922 [==============================] - 0s 4us/sample - loss: 3166981.4317\n",
      "Epoch 94/300\n",
      "2922/2922 [==============================] - 0s 4us/sample - loss: 3157365.1059\n",
      "Epoch 95/300\n",
      "2922/2922 [==============================] - 0s 4us/sample - loss: 3147959.7902\n",
      "Epoch 96/300\n",
      "2922/2922 [==============================] - 0s 5us/sample - loss: 3138705.9933\n",
      "Epoch 97/300\n",
      "2922/2922 [==============================] - 0s 4us/sample - loss: 3129441.9507\n",
      "Epoch 98/300\n",
      "2922/2922 [==============================] - 0s 4us/sample - loss: 3120249.5707\n",
      "Epoch 99/300\n",
      "2922/2922 [==============================] - 0s 7us/sample - loss: 3111262.6869\n",
      "Epoch 100/300\n",
      "2922/2922 [==============================] - 0s 4us/sample - loss: 3102283.2144\n",
      "Epoch 101/300\n",
      "2922/2922 [==============================] - 0s 4us/sample - loss: 3093557.9911\n",
      "Epoch 102/300\n",
      "2922/2922 [==============================] - 0s 5us/sample - loss: 3084725.2118\n",
      "Epoch 103/300\n",
      "2922/2922 [==============================] - 0s 4us/sample - loss: 3076086.0885\n",
      "Epoch 104/300\n",
      "2922/2922 [==============================] - 0s 5us/sample - loss: 3067520.0618\n",
      "Epoch 105/300\n",
      "2922/2922 [==============================] - 0s 5us/sample - loss: 3059090.5873\n",
      "Epoch 106/300\n",
      "2922/2922 [==============================] - 0s 5us/sample - loss: 3050785.2556\n",
      "Epoch 107/300\n",
      "2922/2922 [==============================] - 0s 4us/sample - loss: 3042400.3000\n",
      "Epoch 108/300\n",
      "2922/2922 [==============================] - 0s 5us/sample - loss: 3034233.7452\n",
      "Epoch 109/300\n",
      "2922/2922 [==============================] - 0s 5us/sample - loss: 3026199.1644\n",
      "Epoch 110/300\n",
      "2922/2922 [==============================] - 0s 5us/sample - loss: 3018113.9264\n",
      "Epoch 111/300\n",
      "2922/2922 [==============================] - 0s 5us/sample - loss: 3010232.8450\n",
      "Epoch 112/300\n",
      "2922/2922 [==============================] - 0s 5us/sample - loss: 3002328.4736\n",
      "Epoch 113/300\n",
      "2922/2922 [==============================] - 0s 5us/sample - loss: 2994594.1540\n",
      "Epoch 114/300\n",
      "2922/2922 [==============================] - 0s 5us/sample - loss: 2986907.0106\n",
      "Epoch 115/300\n",
      "2922/2922 [==============================] - 0s 5us/sample - loss: 2979497.5299\n",
      "Epoch 116/300\n",
      "2922/2922 [==============================] - 0s 5us/sample - loss: 2971916.4988\n",
      "Epoch 117/300\n",
      "2922/2922 [==============================] - 0s 5us/sample - loss: 2964438.6342\n",
      "Epoch 118/300\n",
      "2922/2922 [==============================] - 0s 4us/sample - loss: 2957078.7372\n",
      "Epoch 119/300\n",
      "2922/2922 [==============================] - 0s 5us/sample - loss: 2949784.7582\n",
      "Epoch 120/300\n",
      "2922/2922 [==============================] - 0s 4us/sample - loss: 2942717.4894\n",
      "Epoch 121/300\n",
      "2922/2922 [==============================] - 0s 4us/sample - loss: 2935478.9497\n",
      "Epoch 122/300\n",
      "2922/2922 [==============================] - 0s 5us/sample - loss: 2928458.2633\n",
      "Epoch 123/300\n",
      "2922/2922 [==============================] - 0s 4us/sample - loss: 2921547.5334\n",
      "Epoch 124/300\n",
      "2922/2922 [==============================] - 0s 5us/sample - loss: 2914587.3925\n",
      "Epoch 125/300\n",
      "2922/2922 [==============================] - 0s 5us/sample - loss: 2907826.7214\n",
      "Epoch 126/300\n",
      "2922/2922 [==============================] - 0s 5us/sample - loss: 2901049.8924\n",
      "Epoch 127/300\n",
      "2922/2922 [==============================] - 0s 5us/sample - loss: 2894463.5400\n",
      "Epoch 128/300\n",
      "2922/2922 [==============================] - 0s 4us/sample - loss: 2887887.8289\n",
      "Epoch 129/300\n",
      "2922/2922 [==============================] - 0s 4us/sample - loss: 2881404.9851\n",
      "Epoch 130/300\n",
      "2922/2922 [==============================] - 0s 5us/sample - loss: 2874900.4201\n",
      "Epoch 131/300\n",
      "2922/2922 [==============================] - 0s 4us/sample - loss: 2868420.6509\n",
      "Epoch 132/300\n",
      "2922/2922 [==============================] - 0s 4us/sample - loss: 2862086.4896\n",
      "Epoch 133/300\n",
      "2922/2922 [==============================] - 0s 4us/sample - loss: 2855902.2834\n",
      "Epoch 134/300\n",
      "2922/2922 [==============================] - 0s 4us/sample - loss: 2849712.1759\n",
      "Epoch 135/300\n",
      "2922/2922 [==============================] - 0s 4us/sample - loss: 2843629.6870\n",
      "Epoch 136/300\n",
      "2922/2922 [==============================] - 0s 4us/sample - loss: 2837579.2401\n",
      "Epoch 137/300\n",
      "2922/2922 [==============================] - 0s 4us/sample - loss: 2831530.2272\n",
      "Epoch 138/300\n",
      "2922/2922 [==============================] - 0s 4us/sample - loss: 2825626.3334\n",
      "Epoch 139/300\n",
      "2922/2922 [==============================] - 0s 4us/sample - loss: 2819829.3550\n",
      "Epoch 140/300\n",
      "2922/2922 [==============================] - 0s 4us/sample - loss: 2813993.7839\n",
      "Epoch 141/300\n",
      "2922/2922 [==============================] - 0s 3us/sample - loss: 2808374.7895\n",
      "Epoch 142/300\n",
      "2922/2922 [==============================] - 0s 5us/sample - loss: 2802648.7753\n",
      "Epoch 143/300\n",
      "2922/2922 [==============================] - 0s 4us/sample - loss: 2797027.7420\n",
      "Epoch 144/300\n",
      "2922/2922 [==============================] - 0s 4us/sample - loss: 2791419.5173\n",
      "Epoch 145/300\n",
      "2922/2922 [==============================] - 0s 5us/sample - loss: 2786101.5556\n",
      "Epoch 146/300\n",
      "2922/2922 [==============================] - 0s 5us/sample - loss: 2780528.0433\n",
      "Epoch 147/300\n",
      "2922/2922 [==============================] - 0s 5us/sample - loss: 2775234.4521\n",
      "Epoch 148/300\n",
      "2922/2922 [==============================] - 0s 5us/sample - loss: 2769898.3937\n",
      "Epoch 149/300\n",
      "2922/2922 [==============================] - 0s 4us/sample - loss: 2764616.2122\n",
      "Epoch 150/300\n",
      "2922/2922 [==============================] - 0s 4us/sample - loss: 2759406.7283\n",
      "Epoch 151/300\n",
      "2922/2922 [==============================] - 0s 4us/sample - loss: 2754257.8700\n",
      "Epoch 152/300\n",
      "2922/2922 [==============================] - 0s 4us/sample - loss: 2749224.6643\n",
      "Epoch 153/300\n",
      "2922/2922 [==============================] - 0s 4us/sample - loss: 2744170.1987\n",
      "Epoch 154/300\n",
      "2922/2922 [==============================] - 0s 4us/sample - loss: 2739215.6730\n",
      "Epoch 155/300\n",
      "2922/2922 [==============================] - 0s 5us/sample - loss: 2734272.1169\n",
      "Epoch 156/300\n",
      "2922/2922 [==============================] - 0s 4us/sample - loss: 2729387.3800\n",
      "Epoch 157/300\n",
      "2922/2922 [==============================] - 0s 5us/sample - loss: 2724532.9935\n",
      "Epoch 158/300\n",
      "2922/2922 [==============================] - 0s 4us/sample - loss: 2719707.3972\n",
      "Epoch 159/300\n",
      "2922/2922 [==============================] - 0s 5us/sample - loss: 2715081.6992\n",
      "Epoch 160/300\n",
      "2922/2922 [==============================] - 0s 5us/sample - loss: 2710351.7724\n",
      "Epoch 161/300\n",
      "2922/2922 [==============================] - 0s 4us/sample - loss: 2705827.7928\n",
      "Epoch 162/300\n",
      "2922/2922 [==============================] - 0s 4us/sample - loss: 2701181.6379\n",
      "Epoch 163/300\n",
      "2922/2922 [==============================] - 0s 4us/sample - loss: 2696717.6701\n",
      "Epoch 164/300\n",
      "2922/2922 [==============================] - 0s 5us/sample - loss: 2692190.7721\n",
      "Epoch 165/300\n",
      "2922/2922 [==============================] - 0s 4us/sample - loss: 2687709.0388\n",
      "Epoch 166/300\n",
      "2922/2922 [==============================] - 0s 5us/sample - loss: 2683314.6081\n",
      "Epoch 167/300\n",
      "2922/2922 [==============================] - 0s 4us/sample - loss: 2678985.4203\n",
      "Epoch 168/300\n",
      "2922/2922 [==============================] - 0s 4us/sample - loss: 2674706.4290\n",
      "Epoch 169/300\n",
      "2922/2922 [==============================] - 0s 5us/sample - loss: 2670480.7375\n",
      "Epoch 170/300\n",
      "2922/2922 [==============================] - 0s 4us/sample - loss: 2666275.9490\n",
      "Epoch 171/300\n",
      "2922/2922 [==============================] - 0s 4us/sample - loss: 2662145.9726\n",
      "Epoch 172/300\n",
      "2922/2922 [==============================] - 0s 5us/sample - loss: 2658030.6514\n",
      "Epoch 173/300\n",
      "2922/2922 [==============================] - 0s 4us/sample - loss: 2653972.5409\n",
      "Epoch 174/300\n",
      "2922/2922 [==============================] - 0s 4us/sample - loss: 2650008.2310\n",
      "Epoch 175/300\n",
      "2922/2922 [==============================] - 0s 4us/sample - loss: 2646037.9120\n",
      "Epoch 176/300\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2922/2922 [==============================] - 0s 5us/sample - loss: 2642072.2495\n",
      "Epoch 177/300\n",
      "2922/2922 [==============================] - 0s 5us/sample - loss: 2638195.8229\n",
      "Epoch 178/300\n",
      "2922/2922 [==============================] - 0s 4us/sample - loss: 2634323.5257\n",
      "Epoch 179/300\n",
      "2922/2922 [==============================] - 0s 5us/sample - loss: 2630487.6923\n",
      "Epoch 180/300\n",
      "2922/2922 [==============================] - 0s 4us/sample - loss: 2626687.5936\n",
      "Epoch 181/300\n",
      "2922/2922 [==============================] - 0s 5us/sample - loss: 2622972.3602\n",
      "Epoch 182/300\n",
      "2922/2922 [==============================] - 0s 5us/sample - loss: 2619315.0046\n",
      "Epoch 183/300\n",
      "2922/2922 [==============================] - 0s 5us/sample - loss: 2615716.9538\n",
      "Epoch 184/300\n",
      "2922/2922 [==============================] - 0s 4us/sample - loss: 2612067.4714\n",
      "Epoch 185/300\n",
      "2922/2922 [==============================] - 0s 4us/sample - loss: 2608460.7127\n",
      "Epoch 186/300\n",
      "2922/2922 [==============================] - 0s 4us/sample - loss: 2605069.0927\n",
      "Epoch 187/300\n",
      "2922/2922 [==============================] - 0s 4us/sample - loss: 2601469.8669\n",
      "Epoch 188/300\n",
      "2922/2922 [==============================] - 0s 4us/sample - loss: 2598002.9530\n",
      "Epoch 189/300\n",
      "2922/2922 [==============================] - 0s 4us/sample - loss: 2594732.1433\n",
      "Epoch 190/300\n",
      "2922/2922 [==============================] - 0s 5us/sample - loss: 2591172.4317\n",
      "Epoch 191/300\n",
      "2922/2922 [==============================] - 0s 5us/sample - loss: 2587849.1574\n",
      "Epoch 192/300\n",
      "2922/2922 [==============================] - 0s 5us/sample - loss: 2584564.7438\n",
      "Epoch 193/300\n",
      "2922/2922 [==============================] - 0s 5us/sample - loss: 2581302.0979\n",
      "Epoch 194/300\n",
      "2922/2922 [==============================] - 0s 5us/sample - loss: 2578079.3246\n",
      "Epoch 195/300\n",
      "2922/2922 [==============================] - 0s 4us/sample - loss: 2574880.1638\n",
      "Epoch 196/300\n",
      "2922/2922 [==============================] - 0s 4us/sample - loss: 2571687.8190\n",
      "Epoch 197/300\n",
      "2922/2922 [==============================] - 0s 4us/sample - loss: 2568504.8135\n",
      "Epoch 198/300\n",
      "2922/2922 [==============================] - 0s 4us/sample - loss: 2565476.8597\n",
      "Epoch 199/300\n",
      "2922/2922 [==============================] - 0s 4us/sample - loss: 2562335.7274\n",
      "Epoch 200/300\n",
      "2922/2922 [==============================] - 0s 4us/sample - loss: 2559291.1355\n",
      "Epoch 201/300\n",
      "2922/2922 [==============================] - 0s 4us/sample - loss: 2556369.1112\n",
      "Epoch 202/300\n",
      "2922/2922 [==============================] - 0s 4us/sample - loss: 2553355.8702\n",
      "Epoch 203/300\n",
      "2922/2922 [==============================] - 0s 5us/sample - loss: 2550424.4967\n",
      "Epoch 204/300\n",
      "2922/2922 [==============================] - 0s 5us/sample - loss: 2547549.7019\n",
      "Epoch 205/300\n",
      "2922/2922 [==============================] - 0s 5us/sample - loss: 2544665.5893\n",
      "Epoch 206/300\n",
      "2922/2922 [==============================] - 0s 4us/sample - loss: 2541771.8800\n",
      "Epoch 207/300\n",
      "2922/2922 [==============================] - 0s 5us/sample - loss: 2538914.3048\n",
      "Epoch 208/300\n",
      "2922/2922 [==============================] - 0s 5us/sample - loss: 2536211.2269\n",
      "Epoch 209/300\n",
      "2922/2922 [==============================] - 0s 5us/sample - loss: 2533415.1622\n",
      "Epoch 210/300\n",
      "2922/2922 [==============================] - 0s 4us/sample - loss: 2530722.1727\n",
      "Epoch 211/300\n",
      "2922/2922 [==============================] - 0s 5us/sample - loss: 2528009.5566\n",
      "Epoch 212/300\n",
      "2922/2922 [==============================] - 0s 5us/sample - loss: 2525280.1451\n",
      "Epoch 213/300\n",
      "2922/2922 [==============================] - 0s 5us/sample - loss: 2522754.9427\n",
      "Epoch 214/300\n",
      "2922/2922 [==============================] - 0s 5us/sample - loss: 2520120.3193\n",
      "Epoch 215/300\n",
      "2922/2922 [==============================] - 0s 4us/sample - loss: 2517448.1554\n",
      "Epoch 216/300\n",
      "2922/2922 [==============================] - 0s 4us/sample - loss: 2514904.0984\n",
      "Epoch 217/300\n",
      "2922/2922 [==============================] - 0s 6us/sample - loss: 2512443.5208\n",
      "Epoch 218/300\n",
      "2922/2922 [==============================] - 0s 5us/sample - loss: 2509881.8084\n",
      "Epoch 219/300\n",
      "2922/2922 [==============================] - 0s 5us/sample - loss: 2507476.5240\n",
      "Epoch 220/300\n",
      "2922/2922 [==============================] - 0s 4us/sample - loss: 2504897.2539\n",
      "Epoch 221/300\n",
      "2922/2922 [==============================] - 0s 5us/sample - loss: 2502480.1576\n",
      "Epoch 222/300\n",
      "2922/2922 [==============================] - 0s 4us/sample - loss: 2500208.5510\n",
      "Epoch 223/300\n",
      "2922/2922 [==============================] - 0s 4us/sample - loss: 2497830.7940\n",
      "Epoch 224/300\n",
      "2922/2922 [==============================] - 0s 5us/sample - loss: 2495481.4867\n",
      "Epoch 225/300\n",
      "2922/2922 [==============================] - 0s 5us/sample - loss: 2493062.6008\n",
      "Epoch 226/300\n",
      "2922/2922 [==============================] - 0s 6us/sample - loss: 2490747.8149\n",
      "Epoch 227/300\n",
      "2922/2922 [==============================] - 0s 5us/sample - loss: 2488526.6831\n",
      "Epoch 228/300\n",
      "2922/2922 [==============================] - 0s 4us/sample - loss: 2486288.1733\n",
      "Epoch 229/300\n",
      "2922/2922 [==============================] - 0s 4us/sample - loss: 2484077.8142\n",
      "Epoch 230/300\n",
      "2922/2922 [==============================] - 0s 4us/sample - loss: 2481861.9158\n",
      "Epoch 231/300\n",
      "2922/2922 [==============================] - 0s 5us/sample - loss: 2479688.0549\n",
      "Epoch 232/300\n",
      "2922/2922 [==============================] - 0s 4us/sample - loss: 2477479.5746\n",
      "Epoch 233/300\n",
      "2922/2922 [==============================] - 0s 5us/sample - loss: 2475377.9342\n",
      "Epoch 234/300\n",
      "2922/2922 [==============================] - 0s 4us/sample - loss: 2473211.4386\n",
      "Epoch 235/300\n",
      "2922/2922 [==============================] - 0s 5us/sample - loss: 2471170.3614\n",
      "Epoch 236/300\n",
      "2922/2922 [==============================] - 0s 5us/sample - loss: 2469058.2527\n",
      "Epoch 237/300\n",
      "2922/2922 [==============================] - 0s 5us/sample - loss: 2467112.3628\n",
      "Epoch 238/300\n",
      "2922/2922 [==============================] - 0s 5us/sample - loss: 2465036.7842\n",
      "Epoch 239/300\n",
      "2922/2922 [==============================] - 0s 4us/sample - loss: 2463179.3029\n",
      "Epoch 240/300\n",
      "2922/2922 [==============================] - 0s 5us/sample - loss: 2461052.1146\n",
      "Epoch 241/300\n",
      "2922/2922 [==============================] - 0s 4us/sample - loss: 2459087.0708\n",
      "Epoch 242/300\n",
      "2922/2922 [==============================] - 0s 6us/sample - loss: 2457140.8018\n",
      "Epoch 243/300\n",
      "2922/2922 [==============================] - 0s 4us/sample - loss: 2455400.6745\n",
      "Epoch 244/300\n",
      "2922/2922 [==============================] - 0s 5us/sample - loss: 2453436.8941\n",
      "Epoch 245/300\n",
      "2922/2922 [==============================] - 0s 4us/sample - loss: 2451566.2411\n",
      "Epoch 246/300\n",
      "2922/2922 [==============================] - 0s 5us/sample - loss: 2449616.0840\n",
      "Epoch 247/300\n",
      "2922/2922 [==============================] - 0s 4us/sample - loss: 2447800.6615\n",
      "Epoch 248/300\n",
      "2922/2922 [==============================] - 0s 5us/sample - loss: 2445989.5043\n",
      "Epoch 249/300\n",
      "2922/2922 [==============================] - 0s 4us/sample - loss: 2444141.5600\n",
      "Epoch 250/300\n",
      "2922/2922 [==============================] - 0s 5us/sample - loss: 2442354.9363\n",
      "Epoch 251/300\n",
      "2922/2922 [==============================] - 0s 5us/sample - loss: 2440653.3099\n",
      "Epoch 252/300\n",
      "2922/2922 [==============================] - 0s 4us/sample - loss: 2438950.9134\n",
      "Epoch 253/300\n",
      "2922/2922 [==============================] - 0s 5us/sample - loss: 2437162.8183\n",
      "Epoch 254/300\n",
      "2922/2922 [==============================] - 0s 4us/sample - loss: 2435550.4907\n",
      "Epoch 255/300\n",
      "2922/2922 [==============================] - 0s 5us/sample - loss: 2433754.6484\n",
      "Epoch 256/300\n",
      "2922/2922 [==============================] - 0s 5us/sample - loss: 2432243.5941\n",
      "Epoch 257/300\n",
      "2922/2922 [==============================] - 0s 4us/sample - loss: 2430443.7274\n",
      "Epoch 258/300\n",
      "2922/2922 [==============================] - 0s 5us/sample - loss: 2428876.8144\n",
      "Epoch 259/300\n",
      "2922/2922 [==============================] - 0s 5us/sample - loss: 2427190.6323\n",
      "Epoch 260/300\n",
      "2922/2922 [==============================] - 0s 4us/sample - loss: 2425616.5406\n",
      "Epoch 261/300\n",
      "2922/2922 [==============================] - 0s 5us/sample - loss: 2424049.0723\n",
      "Epoch 262/300\n",
      "2922/2922 [==============================] - 0s 5us/sample - loss: 2422556.3365\n",
      "Epoch 263/300\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2922/2922 [==============================] - 0s 4us/sample - loss: 2420956.8198\n",
      "Epoch 264/300\n",
      "2922/2922 [==============================] - 0s 5us/sample - loss: 2419388.6582\n",
      "Epoch 265/300\n",
      "2922/2922 [==============================] - 0s 4us/sample - loss: 2417918.6330\n",
      "Epoch 266/300\n",
      "2922/2922 [==============================] - 0s 5us/sample - loss: 2416354.3595\n",
      "Epoch 267/300\n",
      "2922/2922 [==============================] - 0s 6us/sample - loss: 2414863.6886\n",
      "Epoch 268/300\n",
      "2922/2922 [==============================] - 0s 4us/sample - loss: 2413426.0108\n",
      "Epoch 269/300\n",
      "2922/2922 [==============================] - 0s 4us/sample - loss: 2412101.7303\n",
      "Epoch 270/300\n",
      "2922/2922 [==============================] - 0s 4us/sample - loss: 2410521.5921\n",
      "Epoch 271/300\n",
      "2922/2922 [==============================] - 0s 5us/sample - loss: 2409165.3598\n",
      "Epoch 272/300\n",
      "2922/2922 [==============================] - 0s 4us/sample - loss: 2407745.5072\n",
      "Epoch 273/300\n",
      "2922/2922 [==============================] - 0s 5us/sample - loss: 2406361.3934\n",
      "Epoch 274/300\n",
      "2922/2922 [==============================] - 0s 5us/sample - loss: 2404933.7647\n",
      "Epoch 275/300\n",
      "2922/2922 [==============================] - 0s 5us/sample - loss: 2403689.4454\n",
      "Epoch 276/300\n",
      "2922/2922 [==============================] - 0s 5us/sample - loss: 2402296.9649\n",
      "Epoch 277/300\n",
      "2922/2922 [==============================] - 0s 5us/sample - loss: 2400992.3438\n",
      "Epoch 278/300\n",
      "2922/2922 [==============================] - 0s 7us/sample - loss: 2399614.4447\n",
      "Epoch 279/300\n",
      "2922/2922 [==============================] - 0s 8us/sample - loss: 2398307.3482\n",
      "Epoch 280/300\n",
      "2922/2922 [==============================] - 0s 6us/sample - loss: 2396985.6617\n",
      "Epoch 281/300\n",
      "2922/2922 [==============================] - 0s 5us/sample - loss: 2395813.7401\n",
      "Epoch 282/300\n",
      "2922/2922 [==============================] - 0s 5us/sample - loss: 2394486.5094\n",
      "Epoch 283/300\n",
      "2922/2922 [==============================] - 0s 5us/sample - loss: 2393226.6623\n",
      "Epoch 284/300\n",
      "2922/2922 [==============================] - 0s 4us/sample - loss: 2391933.3332\n",
      "Epoch 285/300\n",
      "2922/2922 [==============================] - 0s 5us/sample - loss: 2390822.2486\n",
      "Epoch 286/300\n",
      "2922/2922 [==============================] - 0s 4us/sample - loss: 2389554.2442\n",
      "Epoch 287/300\n",
      "2922/2922 [==============================] - 0s 5us/sample - loss: 2388372.3609\n",
      "Epoch 288/300\n",
      "2922/2922 [==============================] - 0s 4us/sample - loss: 2387196.0372\n",
      "Epoch 289/300\n",
      "2922/2922 [==============================] - 0s 5us/sample - loss: 2386029.8438\n",
      "Epoch 290/300\n",
      "2922/2922 [==============================] - 0s 4us/sample - loss: 2384816.7243\n",
      "Epoch 291/300\n",
      "2922/2922 [==============================] - 0s 4us/sample - loss: 2383706.6889\n",
      "Epoch 292/300\n",
      "2922/2922 [==============================] - 0s 4us/sample - loss: 2382562.4483\n",
      "Epoch 293/300\n",
      "2922/2922 [==============================] - 0s 5us/sample - loss: 2381506.4312\n",
      "Epoch 294/300\n",
      "2922/2922 [==============================] - 0s 6us/sample - loss: 2380375.9846\n",
      "Epoch 295/300\n",
      "2922/2922 [==============================] - 0s 5us/sample - loss: 2379185.3554\n",
      "Epoch 296/300\n",
      "2922/2922 [==============================] - 0s 5us/sample - loss: 2378109.6632\n",
      "Epoch 297/300\n",
      "2922/2922 [==============================] - 0s 5us/sample - loss: 2377042.4107\n",
      "Epoch 298/300\n",
      "2922/2922 [==============================] - 0s 4us/sample - loss: 2375971.6620\n",
      "Epoch 299/300\n",
      "2922/2922 [==============================] - 0s 5us/sample - loss: 2374936.4933\n",
      "Epoch 300/300\n",
      "2922/2922 [==============================] - 0s 5us/sample - loss: 2373952.7185\n",
      "학습이 종료되었습니다.\n"
     ]
    }
   ],
   "source": [
    "# Tensorflow Ver2.\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from pandas.io.parsers import read_csv\n",
    "\n",
    "data = read_csv('price data.csv', sep=',')  # csv파일 읽어오기\n",
    "\n",
    "xy = np.array(data, dtype=np.float32)    # 행렬 형태로 저장하기\n",
    "\n",
    "# 4개의 변인을 입력을 받는다.\n",
    "x_data = xy[:, 1:-1]   # slicing x\n",
    "\n",
    "# 가격 값을 입력 받는다.\n",
    "y_data = xy[:, [-1]]   # slicing y\n",
    "\n",
    "model = tf.keras.models.Sequential()\n",
    "model.add(tf.keras.layers.Dense(1, input_dim=4))\n",
    "\n",
    "# optimizer\n",
    "# 확률적 경사하강법\n",
    "sgd = tf.keras.optimizers.SGD(lr=0.000005)\n",
    "\n",
    "# cost / loss function\n",
    "model.compile(loss='mse', optimizer=sgd)\n",
    "\n",
    "# 학습시키기\n",
    "# batch_size는 몇장을 보고 맞춰보는지 확인하는 것이다.\n",
    "history = model.fit(x_data, y_data, epochs=300, batch_size=128)\n",
    "\n",
    "print(\"학습이 종료되었습니다.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\lkjs8\\AppData\\Roaming\\Python\\Python38\\site-packages\\tensorflow\\python\\keras\\engine\\training_v1.py:2048: Model.state_updates (from tensorflow.python.keras.engine.training) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "This property should not be used in TensorFlow 2.0, as updates are applied automatically.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "2373185.24721937"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.evaluate(x_data, y_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_keys(['loss'])\n"
     ]
    }
   ],
   "source": [
    "print(history.history.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "loss = history.history['loss']\n",
    "epochs = range(1, len(loss) + 1)            # for x, use len(loss)\n",
    "\n",
    "plt.plot(epochs, loss, 'r', label='Training loss')\n",
    "plt.title('Training and Accuracy')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "학습된 모델을 저장했습니다.\n"
     ]
    }
   ],
   "source": [
    "# 학습된 모델을 저장한다.\n",
    "model.save('./saved.ckpt')\n",
    "print(\"학습된 모델을 저장했습니다.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 참고자료"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. 선형회귀 위키백과(https://ko.wikipedia.org/wiki/%EC%84%A0%ED%98%95_%ED%9A%8C%EA%B7%80)\n",
    "1. 안경잡이개발자  \n",
    "https://ndb796.tistory.com/124\n",
    "1. 텐서플로우 기본 문법  \n",
    "https://gdyoon.tistory.com/8  \n",
    "https://gdyoon.tistory.com/5  \n",
    "https://zetawiki.com/wiki/%ED%85%90%EC%84%9C%ED%94%8C%EB%A1%9C%EC%9A%B0_random_normal()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
