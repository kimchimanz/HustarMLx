{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2020-1 모두를 위한 인공지능 활용\n",
    "\n",
    "## GEK-10109\n",
    "\n",
    "\n",
    "### 프로젝트 7조\n",
    "### 21500615 장관우 21900410 신호철 22000131 김예나\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 예비 보고서 \n",
    "1. 프로젝트 선정계기\n",
    "\n",
    "    a. 중국 우한에서부터 시작된 코로나 바이러스는 접촉 또는 비말에 의해서 감염된다.\n",
    "    하지만 마스크를 착용하면 재채기나 기침으로 인한 바이러스 노출 위험을 낮추어 코로나 바이러스를\n",
    "    조금이나마 예방할 수 있다\n",
    "    \n",
    "    b. 인공지능 기술을 활용하여 접촉하지 않고도 마스크 착용 유무를 판단 하여 코로나 예방에 도움이 되고자한다.\n",
    "    \n",
    "    \n",
    "2. 프로젝트 개요\n",
    "\n",
    "\n",
    "    a. 활용하는 데이터는 Kaggle에 있는 Data set으로, 마스크를 착용한 사람과 마스크를 미 착용한 사람의 이미지에 대한 데이터를 활용한다.\n",
    "    \n",
    "    b. CNN(Convolutional  Neural Network)을 이용한 이미지 학습 모델 생성, Classification(착용/ 미착용)을 활용한다.\n",
    "    \n",
    "    c. dlib와 openCV를 활용하여, 컴퓨터 비전응 이용한 영상처리와 얼굴인식을 하고, 학습 모델을 통해서 마스크 착용자와 마스크 비 착용자를 구별한다.\n",
    "\n",
    "3. 기대효과\n",
    "\n",
    "    a. 마스크를 착용하지 않은 사람들은 관리자에게 알려 건물의 출입을 금할 수 있게 한다.\n",
    "    \n",
    "    b. 집단 감염에 취약한 시설들(대형병원, 종교시설, 유흥업소 등등)에 마스크를 착용하지 않은 사람들에 대한 입장 제한을 통해\n",
    "    집단 감염을 예방할 수 있다.\n",
    "    \n",
    "    c. 사람들에게 마스크 착용에 대한 경각심을 불러 일으킬 수 있다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Classfication 학습하기\n",
    "\n",
    "다음 단계로 학습을 진행한다.\n",
    "\n",
    "1. 데이터 전처리 + 정규화(Normalization)\n",
    "2. CNN 모델 정의\n",
    "3. 손실함수(Loss Function)와 Optimizer 정의\n",
    "4. 학습 (Train)\n",
    "5. 평가 (Validate)\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 프로젝트 구성\n",
    "\n",
    "![Diagram](./Diagram.png)\n",
    "\n",
    "1. CNN과 마스크 데이터셋을 이용하여 Classification 모델을 생성\n",
    "2. Open CV의 얼굴인식 라이브러리를 이용하여, 얼굴을 인식\n",
    "3. 얼굴 이미지를 추출하여, 얼굴 이미지를 PreProcessing과정을 통해, 이미지 가공 후, 모델에 투입\n",
    "4. 0 - Mask 1- Non-Mask 라벨을 통해, Classfication 결과를 화면에 출력\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 모듈 버전 확인"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.5.0\n",
      "0.6.0\n",
      "2.3.1\n",
      "4.2.0\n",
      "1.17.2\n"
     ]
    }
   ],
   "source": [
    "import torch #1.5.0\n",
    "import torchvision #0.6.0\n",
    "import keras #2.3.1\n",
    "import cv2 #4.2.0\n",
    "import numpy as np #1.17.2\n",
    "\n",
    "print(torch.__version__) \n",
    "print(torchvision.__version__)\n",
    "print(keras.__version__)\n",
    "print(cv2.__version__)\n",
    "print(np.__version__)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dataset\n",
    "현재 사용하는 데이터 셋 https://www.kaggle.com/ahmetfurkandemr/mask-datasets-v1/data\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "Data _ Mask\n",
    "![Mask1](./Mask_Datasets/Train/Mask/1.PNG)\n",
    "![Mask2](./Mask_Datasets/Train/Mask/15.PNG)\n",
    "Data _ No MASK \n",
    "![Mask3](./Mask_Datasets/Train/No_Mask/1.PNG)\n",
    "![Mask4](./Mask_Datasets/Train/No_Mask/250.PNG)\n",
    "데이터 셋 구성\n",
    "\n",
    "Training Data Set\n",
    "Mask Data 350개\n",
    "Non-Mask Data 400개\n",
    "\n",
    "Validate Data Set\n",
    "Mask Data 150개\n",
    "Non-Mask Data 200개\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 데이터 전처리 및 정규화\n",
    "현재 있는 이미지 데이터를 보다시피, 크기가 모두 다르기 때문에, 데이터를 사이즈를 일정한 크기로 Resize시켜야 한다. 그리고 Pytorch에서 학습을 진행시키기 위해서 numpy array형태에서 Torch Tensor로 변환, 학습을 위한 정규화 또한 필요하다. \n",
    "torchvision 모듈의 transforms class의 Compose 메소드를 이용하여, 이러한 데이터 전처리 작업을 진행을 위한 transform형태를 정한다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision import transforms\n",
    "import torch\n",
    "import torchvision\n",
    "\n",
    "\n",
    "trans = transforms.Compose([transforms.Resize((224,224)),\n",
    "                           transforms.ToTensor(),\n",
    "                           transforms.Normalize((0.485,0.456,0.406), (0.229, 0.224, 0.225))\n",
    "                           ])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "위에서 정한 Transform형태에 따라 데이터를 torchvision의 Dataset형태로 데이터를 변환시킨다. 이때, Train/Validation 폴더에 있는 Mask와 No_Mask가 분류가 된다. (Label1 = 0 Mask), (Label2 = 1 No_mask)\n",
    "Train_set 변수에 DataLoader형태로 데이터를 변수를 정한다. 이때, batch_size와 shuffle num_workers에 대한 정보를 넘긴다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader, Dataset\n",
    "train_data =  torchvision.datasets.ImageFolder(root='./Mask_Datasets/train/', transform = trans)\n",
    "test_data = torchvision.datasets.ImageFolder(root='./Mask_Datasets/Validation', transform = trans)\n",
    "train_set = DataLoader(dataset = train_data, batch_size = 8, shuffle = True, num_workers=2)\n",
    "test_set = DataLoader(dataset = test_data, batch_size = len(test_data))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Neural Net 만들기\n",
    "\n",
    "신경망에 1채널 이미지만 처리할수 있는 신경망을 3채널로 처리 -> 6채널로 처리 하도록 변경,  \n",
    "44944 ->120 -> 2(Mask / No Mask)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "class CNN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(CNN, self).__init__()\n",
    "        self.layer1 = nn.Sequential(\n",
    "            nn.Conv2d(3,6,5),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(2),\n",
    "        )\n",
    "        self.layer2 = nn.Sequential(\n",
    "            nn.Conv2d(6,16,5),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(2),\n",
    "        )\n",
    "        self.layer3 = nn.Sequential(\n",
    "            nn.Linear(44944, 120),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(120,2)\n",
    "        )\n",
    "        \n",
    "    def forward(self, x):\n",
    "        out = self.layer1(x)\n",
    "        out = self.layer2(out)\n",
    "        out = out.view(out.shape[0], -1)\n",
    "        out = self.layer3(out)\n",
    "        return out\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "net = CNN()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 확인\n",
    "test_input = torch.Tensor(3,3,224,224) # batchsize, RGB channel, W, H\n",
    "test_out = net(test_input)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 손실함수와 Optimizer 정의\n",
    "손실함수는 Cross-Entropy loss를 사용,\n",
    "Optimizer는 SGD를 사용하였다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.optim as optim\n",
    "optimizer = optim.SGD(net.parameters(), lr=0.001, momentum=0.9)\n",
    "loss_func = nn.CrossEntropyLoss()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 신경망 학습\n",
    "7번 epoch으로 학습을 시킨다.\n",
    "학습이 다되면, net.prm파일로 모델을 저장한다.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch:1] cost = 0.5129538774490356\n",
      "[Epoch:2] cost = 0.1684643030166626\n",
      "[Epoch:3] cost = 0.05146336555480957\n",
      "[Epoch:4] cost = 0.0310315303504467\n",
      "[Epoch:5] cost = 0.022869283333420753\n",
      "[Epoch:6] cost = 0.036764949560165405\n",
      "[Epoch:7] cost = 0.013002562336623669\n",
      "Learning Finished!\n"
     ]
    }
   ],
   "source": [
    "total_batch = len(train_set)\n",
    "\n",
    "\n",
    "epochs = 7\n",
    "for epoch in range(epochs):\n",
    "    avg_cost = 0.0\n",
    "    for num, data in enumerate(train_set):\n",
    "        imgs, labels = data\n",
    "        imgs = imgs\n",
    "        labels = labels\n",
    "        optimizer.zero_grad() #변화도를 0으로 만든다.\n",
    "        out = net(imgs)  \n",
    "        loss = loss_func(out, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        avg_cost += loss / total_batch\n",
    "        \n",
    "    print('[Epoch:{}] cost = {}'.format(epoch+1, avg_cost))\n",
    "print('Learning Finished!')\n",
    "params = net.state_dict()\n",
    "torch.save(params, \"net.prm\", pickle_protocol=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 모델 로드\n",
    "혹시 모델이 파일로 있다면 불러올수있다"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "params = torch.load(\"net.prm\", map_location = \"cpu\")\n",
    "net.load_state_dict(params)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 모델 평가 Validation\n",
    "정확도를 확인하여 MASK 와 NO_Mask를 얼마나 잘 구분할수있는지 평가한다"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.9885714054107666\n"
     ]
    }
   ],
   "source": [
    "with torch.no_grad():\n",
    "    for num, data in enumerate(test_set):\n",
    "        imgs, label = data\n",
    "        imgs = imgs\n",
    "        label = label\n",
    "        \n",
    "        \n",
    "        prediction = net(imgs)\n",
    "        \n",
    "        correct_prediction = torch.argmax(prediction, 1) == label\n",
    "        \n",
    "        accuracy = correct_prediction.float().mean()\n",
    "        print('Accuracy:', accuracy.item())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 얼굴인식 OpenCV\n",
    "cv2에 있는 라이브러리중 cascadeClassfier와 얼굴인식 haarcascade를 통해서 얼굴을 인식한다.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from keras.models import load_model\n",
    "import cv2\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "params = torch.load(\"net.prm\", map_location = \"cpu\")\n",
    "net.load_state_dict(params) \n",
    "face_clsfr=cv2.CascadeClassifier('haarcascade_frontalface_default.xml')\n",
    "\n",
    "source=cv2.VideoCapture(0)\n",
    "\n",
    "labels_dict={0:'with_mask',1:'without_mask'}\n",
    "color_dict={0:(0,255,0),1:(0,0,255)}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-13-f74257568924>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      3\u001b[0m     \u001b[0mret\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mimg\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msource\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0mgray\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcv2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcvtColor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimg\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mcv2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mCOLOR_BGR2GRAY\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m     \u001b[0mfaces\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mface_clsfr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdetectMultiScale\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgray\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m1.3\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m5\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mw\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mh\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mfaces\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "while(True):\n",
    "\n",
    "    ret,img=source.read()\n",
    "    gray=cv2.cvtColor(img,cv2.COLOR_BGR2GRAY)\n",
    "    faces=face_clsfr.detectMultiScale(gray,1.3,5)  \n",
    "\n",
    "    for x,y,w,h in faces:\n",
    "    \n",
    "        face_img=img[y:y+w,x:x+w]\n",
    "        \n",
    "        resized=cv2.resize(face_img,(224,224))\n",
    "        \n",
    "        normalized=resized/255.0\n",
    "        reshaped=np.reshape(normalized,(1,3,224,224))\n",
    "        reshaped = reshaped.astype(np.float32)\n",
    "        \n",
    "        reshaped = torch.from_numpy(reshaped)\n",
    "        \n",
    "        result = net(reshaped)\n",
    "        \n",
    "        label = torch.argmax(result, 1)[0]\n",
    "        label = label.item()\n",
    "        #label=np.argmax(result, axis=1)[0]\n",
    "       \n",
    "        cv2.rectangle(img,(x,y),(x+w,y+h),color_dict[label],2)\n",
    "        cv2.rectangle(img,(x,y-40),(x+w,y),color_dict[label],-1)\n",
    "        cv2.putText(img, labels_dict[label], (x, y-10),cv2.FONT_HERSHEY_SIMPLEX,0.8,(255,255,255),2)\n",
    "        \n",
    "        \n",
    "    cv2.imshow('LIVE',img)\n",
    "    key=cv2.waitKey(1)\n",
    "    \n",
    "    if(key==27):\n",
    "        break\n",
    "        \n",
    "cv2.destroyAllWindows()\n",
    "source.release()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Reference\n",
    "\n",
    "https://tutorials.pytorch.kr/beginner/blitz/cifar10_tutorial.html\n",
    "\n",
    "\n",
    "https://towardsdatascience.com/real-time-face-mask-detector-with-tensorflow-keras-and-opencv-38b552660b64"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "PyTorch",
   "language": "python",
   "name": "torchvision"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
